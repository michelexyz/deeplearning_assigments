{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [1, -1]\n",
      "W: [[1, 1, 1], [-1, -1, -1]]\n",
      "k: [2, 2, 2]\n",
      "z: [0.8807970779778823, 0.8807970779778823, 0.8807970779778823]\n",
      "V: [[1, 1], [-1, -1], [-1, -1]]\n",
      "o: [-0.8807970779778823, -0.8807970779778823]\n",
      "y: [0.5, 0.5]\n",
      "loss: 0.6931471805599453\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "# Assigment 1 forwad pass and backpropagation without using any library\n",
    "\n",
    "x = [1,-1]\n",
    "t = [1,0]\n",
    "\n",
    "w = [[1,1,1],[-1,-1,-1]]\n",
    "b = [0,0,0]\n",
    "\n",
    "v = [[1,1],[-1,-1],[-1,-1]]\n",
    "c = [0,0]\n",
    "\n",
    "\n",
    "# forward pass\n",
    "\n",
    "print(f\"x: {x}\")\n",
    "\n",
    "# first layer\n",
    "print(f\"W: {w}\")\n",
    "\n",
    "k = [sum([x[i]*w[i][j] for i in range(len(x))]) + b[j] for j in range(len(b))]\n",
    "\n",
    "print(f\"k: {k}\")\n",
    "\n",
    "# sigmoid layer\n",
    "z = [1/(1+math.exp(-k[i])) for i in range(len(k))]\n",
    "\n",
    "print(f\"z: {z}\")\n",
    "\n",
    "# second layer\n",
    "print(f\"V: {v}\")\n",
    "\n",
    "o = [sum([z[i]*v[i][j] for i in range(len(z))]) + c[j] for j in range(len(c))]\n",
    "\n",
    "print(f\"o: {o}\")\n",
    "\n",
    "# softmax layer\n",
    "y = [math.exp(o[i])/sum([math.exp(o[j]) for j in range(len(o))]) for i in range(len(o))]\n",
    "\n",
    "print(f\"y: {y}\")\n",
    "\n",
    "# cross entropy loss\n",
    "loss = -sum([t[i]*math.log(y[i]) for i in range(len(t))])\n",
    "\n",
    "print(f\"loss: {loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dy: [-2.0, -0.0]\n",
      "do: [-0.5, 0.5]\n",
      "dv: [[-0.44039853898894116, 0.44039853898894116], [-0.44039853898894116, 0.44039853898894116], [-0.44039853898894116, 0.44039853898894116]]\n",
      "dc: [-0.5, 0.5]\n",
      "dz: [0.0, 0.0, 0.0]\n",
      "dk: [0.0, 0.0, 0.0]\n",
      "dw: [[0.0, 0.0, 0.0], [-0.0, -0.0, -0.0]]\n",
      "db: [0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "# backward pass\n",
    "# derivative of loss function with respect to yi\n",
    "dy = [(-1/y[i])*t[i] for i in range(len(y))]\n",
    "\n",
    "print(f\"dy: {dy}\")\n",
    "\n",
    "# derivative of loss function with respect to oi\n",
    "do = [sum([dy[j] * (y[j]*(1-y[j]) if j==i else -y[j]*y[i]) for j in range(len(dy))]) for i in range(len(o))]\n",
    "\n",
    "print(f\"do: {do}\")\n",
    "\n",
    "# derivative of loss function with respect to v\n",
    "dv = [[do[j]*z[i] for j in range(len(do))] for i in range(len(z))]\n",
    "\n",
    "print(f\"dv: {dv}\")\n",
    "\n",
    "# derivative of loss function with respect to ci\n",
    "dc = [do[i]  for i in range(len(c))]\n",
    "\n",
    "print(f\"dc: {dc}\")\n",
    "\n",
    "# derivative of loss function with respect to zi\n",
    "dz = [sum([do[j]*v[i][j] for j in range(len(do))]) for i in range(len(v))]\n",
    "\n",
    "print(f\"dz: {dz}\")\n",
    "\n",
    "# derivative of loss function with respect to ki\n",
    "dk = [dz[i]*z[i]*(1-z[i]) for i in range(len(z))]\n",
    "\n",
    "print(f\"dk: {dk}\")\n",
    "\n",
    "# derivative of loss function with respect to wi\n",
    "dw = [[dk[j]*x[i] for j in range(len(dk))] for i in range(len(x))]\n",
    "\n",
    "print(f\"dw: {dw}\")\n",
    "\n",
    "# derivative of loss function with respect to bi\n",
    "db = [dk[i] for i in range(len(b))]\n",
    "\n",
    "print(f\"db: {db}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dw: [[0.0, 0.0, 0.0], [-0.0, -0.0, -0.0]]\n",
      "db: [0.0, 0.0, 0.0]\n",
      "dc: [-0.5, 0.5]\n",
      "dv: [[-0.44039853898894116, 0.44039853898894116], [-0.44039853898894116, 0.44039853898894116], [-0.44039853898894116, 0.44039853898894116]]\n",
      "Epoch 1, Average Loss: 0.7271016556975978\n",
      "Epoch 2, Average Loss: 0.6833104722571401\n",
      "Epoch 3, Average Loss: 0.6478012119807957\n",
      "Epoch 4, Average Loss: 0.6179929318115058\n",
      "Epoch 5, Average Loss: 0.5921725289488446\n",
      "Epoch 6, Average Loss: 0.5692155385385275\n",
      "Epoch 7, Average Loss: 0.5483821001909135\n",
      "Epoch 8, Average Loss: 0.5291803634833332\n",
      "Epoch 9, Average Loss: 0.51127833919219\n",
      "Epoch 10, Average Loss: 0.49444772247397856\n",
      "Epoch 11, Average Loss: 0.4785281028622107\n",
      "Epoch 12, Average Loss: 0.4634040280354398\n",
      "Epoch 13, Average Loss: 0.4489901738010207\n",
      "Epoch 14, Average Loss: 0.43522165760908116\n",
      "Epoch 15, Average Loss: 0.4220476452513322\n",
      "Epoch 16, Average Loss: 0.40942708780831905\n",
      "Epoch 17, Average Loss: 0.39732585128852094\n",
      "Epoch 18, Average Loss: 0.38571476632982704\n",
      "Epoch 19, Average Loss: 0.3745682918249306\n",
      "Epoch 20, Average Loss: 0.36386359203832297\n",
      "Epoch 21, Average Loss: 0.3535798946169369\n",
      "Epoch 22, Average Loss: 0.343698040895963\n",
      "Epoch 23, Average Loss: 0.3342001687418253\n",
      "Epoch 24, Average Loss: 0.32506948727157553\n",
      "Epoch 25, Average Loss: 0.3162901155579051\n",
      "Epoch 26, Average Loss: 0.30784696604834566\n",
      "Epoch 27, Average Loss: 0.2997256592972444\n",
      "Epoch 28, Average Loss: 0.29191246064065324\n",
      "Epoch 29, Average Loss: 0.28439423223515226\n",
      "Epoch 30, Average Loss: 0.2771583958277749\n",
      "Epoch 31, Average Loss: 0.27019290299023246\n",
      "Epoch 32, Average Loss: 0.2634862105149458\n",
      "Epoch 33, Average Loss: 0.25702725935434245\n",
      "Epoch 34, Average Loss: 0.25080545597164394\n",
      "Epoch 35, Average Loss: 0.24481065531856191\n",
      "Epoch 36, Average Loss: 0.23903314490304128\n",
      "Epoch 37, Average Loss: 0.23346362958669453\n",
      "Epoch 38, Average Loss: 0.22809321687681405\n",
      "Epoch 39, Average Loss: 0.22291340256613879\n",
      "Epoch 40, Average Loss: 0.21791605663508698\n",
      "Epoch 41, Average Loss: 0.21309340937340226\n",
      "Epoch 42, Average Loss: 0.2084380377064192\n",
      "Epoch 43, Average Loss: 0.2039428517294205\n",
      "Epoch 44, Average Loss: 0.19960108146463587\n",
      "Epoch 45, Average Loss: 0.19540626386148055\n",
      "Epoch 46, Average Loss: 0.19135223006314475\n",
      "Epoch 47, Average Loss: 0.18743309296278315\n",
      "Epoch 48, Average Loss: 0.18364323507111208\n",
      "Epoch 49, Average Loss: 0.17997729671481089\n",
      "Epoch 50, Average Loss: 0.17643016458216507\n",
      "Epoch 51, Average Loss: 0.17299696062918402\n",
      "Epoch 52, Average Loss: 0.16967303135617723\n",
      "Epoch 53, Average Loss: 0.16645393746164006\n",
      "Epoch 54, Average Loss: 0.16333544387736143\n",
      "Epoch 55, Average Loss: 0.16031351018596618\n",
      "Epoch 56, Average Loss: 0.15738428141970306\n",
      "Epoch 57, Average Loss: 0.15454407923717015\n",
      "Epoch 58, Average Loss: 0.15178939347282833\n",
      "Epoch 59, Average Loss: 0.14911687405260882\n",
      "Epoch 60, Average Loss: 0.14652332326761275\n",
      "Epoch 61, Average Loss: 0.1440056883968466\n",
      "Epoch 62, Average Loss: 0.14156105466909033\n",
      "Epoch 63, Average Loss: 0.1391866385533473\n",
      "Epoch 64, Average Loss: 0.13687978136684548\n",
      "Epoch 65, Average Loss: 0.13463794318922828\n",
      "Epoch 66, Average Loss: 0.13245869707138153\n",
      "Epoch 67, Average Loss: 0.1303397235272526\n",
      "Epoch 68, Average Loss: 0.1282788052970303\n",
      "Epoch 69, Average Loss: 0.12627382237014478\n",
      "Epoch 70, Average Loss: 0.1243227472567036\n",
      "Epoch 71, Average Loss: 0.12242364049619503\n",
      "Epoch 72, Average Loss: 0.1205746463925419\n",
      "Epoch 73, Average Loss: 0.11877398896488808\n",
      "Epoch 74, Average Loss: 0.1170199681038144\n",
      "Epoch 75, Average Loss: 0.11531095592302062\n",
      "Epoch 76, Average Loss: 0.11364539329686696\n",
      "Epoch 77, Average Loss: 0.11202178657452705\n",
      "Epoch 78, Average Loss: 0.11043870446187572\n",
      "Epoch 79, Average Loss: 0.10889477506260045\n",
      "Epoch 80, Average Loss: 0.1073886830703947\n",
      "Epoch 81, Average Loss: 0.10591916710445246\n",
      "Epoch 82, Average Loss: 0.1044850171808405\n",
      "Epoch 83, Average Loss: 0.10308507231267094\n",
      "Epoch 84, Average Loss: 0.10171821823234065\n",
      "Epoch 85, Average Loss: 0.10038338522942611\n",
      "Epoch 86, Average Loss: 0.09907954609814716\n",
      "Epoch 87, Average Loss: 0.09780571418861429\n",
      "Epoch 88, Average Loss: 0.09656094155637304\n",
      "Epoch 89, Average Loss: 0.09534431720503887\n",
      "Epoch 90, Average Loss: 0.09415496541708868\n",
      "Epoch 91, Average Loss: 0.09299204416813418\n",
      "Epoch 92, Average Loss: 0.09185474362024874\n",
      "Epoch 93, Average Loss: 0.09074228469015522\n",
      "Epoch 94, Average Loss: 0.08965391768830813\n",
      "Epoch 95, Average Loss: 0.08858892102511529\n",
      "Epoch 96, Average Loss: 0.08754659998074688\n",
      "Epoch 97, Average Loss: 0.08652628553517511\n",
      "Epoch 98, Average Loss: 0.08552733325526438\n",
      "Epoch 99, Average Loss: 0.08454912223591418\n",
      "Epoch 100, Average Loss: 0.0835910540924093\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "class SimpleNeuralNetwork:\n",
    "    def __init__(self, w, b, v, c):\n",
    "        # Initialize weights and biases\n",
    "        self.w = w\n",
    "        self.b = b\n",
    "        self.v = v\n",
    "        self.c = c\n",
    "    \n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + math.exp(-x))\n",
    "\n",
    "    def forward_pass(self, x):\n",
    "        # First layer\n",
    "        k = [sum(x[i] * self.w[i][j] for i in range(len(x))) + self.b[j] for j in range(len(self.b))]\n",
    "        # Sigmoid layer\n",
    "        z = [self.sigmoid(k[i]) for i in range(len(k))]\n",
    "        # Second layer\n",
    "        o = [sum(z[i] * self.v[i][j] for i in range(len(z))) + self.c[j] for j in range(len(self.c))]\n",
    "        # Softmax layer\n",
    "        y = [math.exp(o[i]) / sum(math.exp(o[j]) for j in range(len(o))) for i in range(len(o))]\n",
    "        return y, z, k, o\n",
    "\n",
    "    def compute_loss(self, y, t):\n",
    "        return -sum(t[i] * math.log(y[i]) for i in range(len(t)))\n",
    "\n",
    "    def backward_pass(self,t, y, o, z, x):\n",
    "        # Derivatives of the loss function\n",
    "        # derivative of loss function with respect to yi\n",
    "        dy = [(-t[i] / y[i]) for i in range(len(y))]\n",
    "\n",
    "        # derivative of loss function with respect to oi\n",
    "        do = [sum([dy[j] * (y[j]*(1-y[j]) if j==i else -y[j]*y[i]) for j in range(len(dy))]) for i in range(len(o))]\n",
    "\n",
    "        # derivative of loss function with respect to v\n",
    "        dv = [[do[j]*z[i] for j in range(len(do))] for i in range(len(z))]\n",
    "\n",
    "        # derivative of loss function with respect to ci\n",
    "        dc = [do[i]  for i in range(len(c))]\n",
    "\n",
    "        # derivative of loss function with respect to zi\n",
    "        dz = [sum([do[j]*self.v[i][j] for j in range(len(do))]) for i in range(len(self.v))]\n",
    "\n",
    "        # derivative of loss function with respect to ki\n",
    "        dk = [dz[i]*z[i]*(1-z[i]) for i in range(len(z))]\n",
    "\n",
    "        # derivative of loss function with respect to wi\n",
    "        dw = [[dk[j]*x[i] for j in range(len(dk))] for i in range(len(x))]\n",
    "\n",
    "        # derivative of loss function with respect to bi\n",
    "        db = [dk[i] for i in range(len(b))]\n",
    "\n",
    "        return dw, db, dc, dv\n",
    "    def update_weights(self, dw, db,dc, dv, learning_rate=0.1):\n",
    "        # Update the weights and biases using the learning rate\n",
    "        for i in range(len(self.w)):\n",
    "            for j in range(len(self.w[i])):\n",
    "                self.w[i][j] -= learning_rate * dw[i][j]\n",
    "        \n",
    "        for i in range(len(self.b)):\n",
    "            self.b[i] -= learning_rate * db[i]\n",
    "\n",
    "        for i in range(len(self.v)):\n",
    "            for j in range(len(self.v[i])):\n",
    "                self.v[i][j] -= learning_rate * dv[i][j]\n",
    "\n",
    "        for i in range(len(self.c)):\n",
    "            self.c[i] -= learning_rate * dc[i]\n",
    "\n",
    "    def train(self, xtrain, ytrain, epochs):\n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "            for index,(x, t) in enumerate(zip(xtrain, ytrain)):\n",
    "                y, z, k, o = self.forward_pass(x)\n",
    "                loss = self.compute_loss(y, t)\n",
    "                dw, db, dc, dv = self.backward_pass(t, y, o, z, x)\n",
    "                # if is first run print derivative\n",
    "                if epoch == 0 and index == 0:\n",
    "                    print(f\"dw: {dw}\")\n",
    "                    print(f\"db: {db}\")\n",
    "                    print(f\"dc: {dc}\")\n",
    "                    print(f\"dv: {dv}\")\n",
    "                self.update_weights(dw, db, dc, dv)\n",
    "                total_loss += loss\n",
    "            avg_loss = total_loss / len(xtrain)\n",
    "            print(f\"Epoch {epoch + 1}, Average Loss: {avg_loss}\")\n",
    "            \n",
    "    def validate(self, xval, yval):\n",
    "        total_loss = 0\n",
    "        correct_predictions = 0\n",
    "        for x, t in zip(xval, yval):\n",
    "            y, _, _, _ = self.forward_pass(x)\n",
    "\n",
    "            # Predicted class is the one with the highest probability\n",
    "            predicted_class = y.index(max(y))\n",
    "\n",
    "            # Actual class is the one with the highest value in the target vector\n",
    "            actual_class = t.index(max(t))\n",
    "\n",
    "            # Increment correct predictions if the classes match\n",
    "            if predicted_class == actual_class:\n",
    "                correct_predictions += 1\n",
    "\n",
    "            loss = self.compute_loss(y, t)\n",
    "            total_loss += loss\n",
    "\n",
    "        avg_loss = total_loss / len(xval)\n",
    "        accuracy = correct_predictions / len(xval)\n",
    "        return avg_loss, accuracy\n",
    "\n",
    "# Example usage\n",
    "x = [[1, -1], [-1, 1]]\n",
    "t = [[1, 0], [0, 1]]\n",
    "w = [[1, 1, 1], [-1, -1, -1]]\n",
    "b = [0, 0, 0]\n",
    "v = [[1, 1], [-1, -1], [-1, -1]]\n",
    "c = [0, 0]\n",
    "nn = SimpleNeuralNetwork(w, b, v, c)\n",
    "nn.train(x, t, 100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w: [[1.3190182013711846, 1.3190182013711844, 1.3190182013711844], [-1.3190182013711846, -1.3190182013711844, -1.3190182013711844]]\n",
      "b: [-0.10298842004189203, -0.102988420041892, -0.102988420041892]\n",
      "v: [[1.9765982502944903, 0.02340174970551011], [-0.023401749705510153, -1.9765982502944905], [-0.023401749705510153, -1.9765982502944905]]\n",
      "c: [-1.2382454716823024, 1.2382454716823015]\n"
     ]
    }
   ],
   "source": [
    "# print weights and biases of the network\n",
    "print(f\"w: {nn.w}\")\n",
    "print(f\"b: {nn.b}\")\n",
    "print(f\"v: {nn.v}\")\n",
    "print(f\"c: {nn.c}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
